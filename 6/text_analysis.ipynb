{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テキスト解析の場合\n",
    "\n",
    "高次元データの例として、テキスト解析をする場合を考えましょう。\n",
    "\n",
    "データとしてはいつもの 20 newsgroups データを使用することにします。TF-IDF 化まで行われたものを利用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "dataset = fetch_20newsgroups(subset='all', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストをベクトル化するのは前回説明した gensim を使うこともできますが、今回は scikit-learn に用意されているTF-IDFベクトル化を使います。\n",
    "\n",
    "文書数は18856で、単語は10000語とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, max_features=10000, min_df=2, stop_words='english')\n",
    "TfIdf = vectorizer.fit_transform(dataset.data)\n",
    "print(TfIdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "\n",
    "KMeans法(MiniBatchKMeans)をとりあえず試してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from time import time\n",
    "km = MiniBatchKMeans(n_clusters=100, init='k-means++', n_init=1, init_size=1000, batch_size=1000)\n",
    "# KMeans に切り替えて実行時間を比較するのも試してみてください\n",
    "#km = KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1)\n",
    "t0 = time()\n",
    "km.fit(TfIdf)\n",
    "print(\"%0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "クラスタリングの性能を評価する指標は幾つかあります。\n",
    "\n",
    "注意：アイテムが０のクラスタがあると、RuntimeWarning: Mean of empty slice.と出ますが気にせず実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "labels = dataset.target # 正解=newsgroup名\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(TfIdf, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピックモデリングを行い次元を削減してみます。今回は `gensim` を使わず scikit-learn の SVD(特異値分解)を使います。クラスタ数は100に固定して、トピック数を変えてSilhouette scoreを比較してみます。(時間がある人はグリッドサーチで、クラスタ数とトピック数をサーチしてみてください。)\n",
    "\n",
    "（アイテムが０のクラスタがあると、`RuntimeWarning: Mean of empty slice.`と出ますが気にせず実行してください。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "n_topics = [2, 5, 10, 20, 50, 100, 200, 500, 1000]\n",
    "silhouettes = []\n",
    "for n_topic in n_topics:\n",
    "    svd = TruncatedSVD(n_topic)\n",
    "    lsi = make_pipeline(svd, Normalizer(copy=False))\n",
    "    X = lsi.fit_transform(TfIdf)\n",
    "    km1 = MiniBatchKMeans(n_clusters=100, init='k-means++', n_init=1, init_size=1000, batch_size=1000)\n",
    "    km1.fit(X)\n",
    "    silhouettes.append(metrics.silhouette_score(X, km1.labels_, sample_size=1000))\n",
    "plt.plot(np.array(n_topics), np.array(silhouettes), 'o-')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トピック数は100程度あれば十分なようなので、クラスタ数を変えて調べてみます。（アイテムが０のクラスタがあると、`RuntimeWarning: Mean of empty slice.`と出ますが気にせず実行してください。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = [2, 5, 7, 10, 15, 20, 30, 50, 70, 100, 150, 200, 300, 500, 700, 1000]\n",
    "\n",
    "svd = TruncatedSVD(100)\n",
    "lsi = make_pipeline(svd, Normalizer(copy=False))\n",
    "X = lsi.fit_transform(TfIdf)\n",
    "\n",
    "silhouettes = []\n",
    "for n_cluster in n_clusters:\n",
    "    km1 = MiniBatchKMeans(n_clusters=n_cluster, init='k-means++', n_init=1, init_size=1000, batch_size=1000)\n",
    "    km1.fit(X)\n",
    "    silhouettes.append(metrics.silhouette_score(TfIdf, km1.labels_, sample_size=1000))\n",
    "plt.plot(np.array(n_clusters), np.array(silhouettes), 'o-')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "クラスタ数は大体 100 程度の値で良さそうでしょうか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biclustering\n",
    "\n",
    "Biclustering は行列を指定した数のブロック対角化された行列へと分解する手法です。\n",
    "\n",
    "先ほどの解析では文書中の単語をトピックモデリングし、そのトピックへの分解を用いてクラスタリングを行う、という二段階に分けて解析を行いました。Biclustering は（文書）x（単語）という行列の要素が Tf-Idf 値である場合、その行列を、縦も横もトピック数の数だけ分割されたブロック行列へと行列を変形します。\n",
    "\n",
    "ライブラリの使い方はほぼ同じで `fit` メソッドを呼ぶだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster.bicluster import SpectralCoclustering\n",
    "cocluster = SpectralCoclustering(n_clusters=20)\n",
    "cocluster.fit(TfIdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの読み出し方は少し違います。`row_labels_`属性は各データの所属するクラスタ番号を返します。`column_labels_` 属性は素性側の所属するクラスタ番号が入っています。\n",
    "\n",
    "人間が読みやすいように、`dataset.target_names`を使ってnewsgroup名に、`vectorizer.get_feature_names()`を使って単語に変換します。\n",
    "\n",
    "結果を見ると、文章と単語とを同時にグループ化しようとしていることが判ります（まぁあまりうまくいってないようにも見えますが。。。）\n",
    "\n",
    "今回はちゃんとストップワードを指定していないので、意味のなさそうな単語(例えばb8g, 5g9p, 7klj, 9f9, 1fpl, bs0t, 2tct, 7kn, bxn, gizwとか)が辞書に現れているようです。実際の案件ではストップワードなどをちゃんと設定する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cocluster.row_labels_)\n",
    "print(cocluster.row_labels_.shape)\n",
    "print(len(dataset.target))\n",
    "print(len(dataset.target_names))\n",
    "for c in range(20): # c=クラスタ番号\n",
    "    totalr, dicr = 0, dict()\n",
    "    totalc, dicc = 0, dict()\n",
    "    for (i,t) in zip(cocluster.row_labels_, dataset.target):\n",
    "        name = dataset.target_names[t]\n",
    "        if (c == i):\n",
    "            if (name in dicr):\n",
    "                dicr[name] += 1\n",
    "            else:\n",
    "                dicr[name] = 1\n",
    "            totalr += 1\n",
    "        else:\n",
    "            pass\n",
    "    for (i,name) in zip(cocluster.column_labels_, vectorizer.get_feature_names()):\n",
    "        if (c == i):\n",
    "            if (name in dicc):\n",
    "                dicc[name] += 1\n",
    "            else:\n",
    "                dicc[name] = 1\n",
    "            totalc += 1\n",
    "        else:\n",
    "            pass\n",
    "    print('cluster: {0}: {1},{2}'.format(c, totalr, totalc))\n",
    "    zsr = [(k,v) for (k,v) in dicr.items()]\n",
    "    zsr.sort(key=lambda t: -t[1])\n",
    "    for k,v in zsr[:3]:\n",
    "        if (totalc != 0):\n",
    "            print('   {0} : {1:.2f}%'.format(k, v/totalc*100))\n",
    "        else:\n",
    "            pass\n",
    "    zsc = [(k,v) for (k,v) in dicc.items()]\n",
    "    zsc.sort(key=lambda t: -t[1])\n",
    "    print('   words={0}'.format(','.join([k for k,v in zsc[:10]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回やっていることは、トピックモデリングと似たことを繰り返したように見えます。顧客と購入アイテムの行列などを用いればレコメンデーションや顧客クラスタリングなどにも使用可能でしょう。\n",
    "\n",
    "あるいは Tf-Idf 行列の代わりにノード間の近接度を表す行列を使えば、ノードのクラスタリングが行われます。\n",
    "\n",
    "色々、使い方を考えてみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
